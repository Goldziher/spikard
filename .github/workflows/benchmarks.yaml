name: Benchmarks

'on':
  # Manual trigger only
  workflow_dispatch:
    inputs:
      language:
        description: 'Language to benchmark (python/node/ruby/rust/all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - python
          - node
          - ruby
          - rust
      workload-suite:
        description: 'Workload suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - json-bodies
          - path-params
          - query-params
          - forms
      duration:
        description: 'Duration per workload in seconds'
        required: false
        default: '30'
        type: string
      concurrency:
        description: 'Concurrent connections per workload'
        required: false
        default: '100'
        type: string
      warmup:
        description: 'Warmup requests per framework'
        required: false
        default: '100'
        type: string
      significance-threshold:
        description: 'Statistical significance threshold (p-value)'
        required: false
        default: '0.05'
        type: string
      comparison-frameworks:
        description: 'Override comparison frameworks (comma-separated, leave empty for defaults)'
        required: false
        default: ''
        type: string

concurrency:
  group: benchmarks-${{ github.ref }}-${{ inputs.language }}
  cancel-in-progress: true

env:
  RUST_BACKTRACE: short
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  CARGO_PROFILE_RELEASE_DEBUG: 0

jobs:
  # Build the benchmark harness once for all jobs
  build-harness:
    name: Build Benchmark Harness
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v6

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: benchmark-harness

      - name: Build benchmark harness
        run: cargo build --manifest-path tools/benchmark-harness/Cargo.toml --release

      - name: Upload harness binary
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-harness-binary
          path: target/release/benchmark-harness
          retention-days: 1

  # Independent benchmark job for Python
  bench-python:
    name: Python Benchmarks
    needs: build-harness
    if: inputs.language == 'all' || inputs.language == 'python'
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
      - uses: actions/checkout@v6

      - name: Download harness binary
        uses: actions/download-artifact@v4
        with:
          name: benchmark-harness-binary
          path: bin

      - name: Make harness executable
        run: chmod +x bin/benchmark-harness && echo "$PWD/bin" >> $GITHUB_PATH

      - uses: ./.github/actions/build-python-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: python
          spikard-framework: spikard-python
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'fastapi,fastapi-granian,litestar,litestar-granian,robyn' }}
          workload-suite: ${{ inputs.workload-suite || 'all' }}
          duration: ${{ inputs.duration || '30' }}
          concurrency: ${{ inputs.concurrency || '100' }}
          warmup: ${{ inputs.warmup || '100' }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8100'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-python-${{ github.run_id }}
          path: results/**/*.json
          retention-days: 90

  # Independent benchmark job for Node.js
  bench-node:
    name: Node.js Benchmarks
    needs: build-harness
    if: inputs.language == 'all' || inputs.language == 'node'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6

      - name: Download harness binary
        uses: actions/download-artifact@v4
        with:
          name: benchmark-harness-binary
          path: bin

      - name: Make harness executable
        run: chmod +x bin/benchmark-harness && echo "$PWD/bin" >> $GITHUB_PATH

      - uses: ./.github/actions/build-node-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: node
          spikard-framework: spikard-node
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'fastify,express,hono' }}
          workload-suite: ${{ inputs.workload-suite || 'all' }}
          duration: ${{ inputs.duration || '30' }}
          concurrency: ${{ inputs.concurrency || '100' }}
          warmup: ${{ inputs.warmup || '100' }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8200'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-node-${{ github.run_id }}
          path: results/**/*.json
          retention-days: 90

  # Independent benchmark job for Ruby
  bench-ruby:
    name: Ruby Benchmarks
    needs: build-harness
    if: inputs.language == 'all' || inputs.language == 'ruby'
    runs-on: ubuntu-latest
    timeout-minutes: 50
    steps:
      - uses: actions/checkout@v6

      - name: Download harness binary
        uses: actions/download-artifact@v4
        with:
          name: benchmark-harness-binary
          path: bin

      - name: Make harness executable
        run: chmod +x bin/benchmark-harness && echo "$PWD/bin" >> $GITHUB_PATH

      - uses: ./.github/actions/build-ruby-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: ruby
          spikard-framework: spikard-ruby
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'hanami-api,roda' }}
          workload-suite: ${{ inputs.workload-suite || 'all' }}
          duration: ${{ inputs.duration || '30' }}
          concurrency: ${{ inputs.concurrency || '100' }}
          warmup: ${{ inputs.warmup || '100' }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8300'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-ruby-${{ github.run_id }}
          path: results/**/*.json
          retention-days: 90

  # Independent benchmark job for Rust
  bench-rust:
    name: Rust Benchmarks
    needs: build-harness
    if: inputs.language == 'all' || inputs.language == 'rust'
    runs-on: ubuntu-latest
    timeout-minutes: 40
    steps:
      - uses: actions/checkout@v6

      - name: Download harness binary
        uses: actions/download-artifact@v4
        with:
          name: benchmark-harness-binary
          path: bin

      - name: Make harness executable
        run: chmod +x bin/benchmark-harness && echo "$PWD/bin" >> $GITHUB_PATH

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: benchmark-rust

      - uses: ./.github/actions/run-benchmarks
        with:
          language: rust
          spikard-framework: spikard-rust
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'axum-baseline' }}
          workload-suite: ${{ inputs.workload-suite || 'all' }}
          duration: ${{ inputs.duration || '30' }}
          concurrency: ${{ inputs.concurrency || '100' }}
          warmup: ${{ inputs.warmup || '100' }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8400'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-rust-${{ github.run_id }}
          path: results/**/*.json
          retention-days: 90

  # Summary report (only runs if at least one benchmark ran)
  report:
    name: Generate Summary Report
    needs: [build-harness, bench-python, bench-node, bench-ruby, bench-rust]
    if: always() && needs.build-harness.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6

      - name: Download all available results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          path: results
          pattern: benchmark-*
          merge-multiple: false

      - name: Create summary report
        run: |
          cat > benchmark-summary.md << 'EOF'
          # ðŸ“Š Spikard Benchmark Results

          ## Run Information

          | Attribute | Value |
          |-----------|-------|
          | **Run ID** | [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) |
          | **Language** | `${{ inputs.language }}` |
          | **Commit** | [`${{ github.sha }}`](https://github.com/${{ github.repository }}/commit/${{ github.sha }}) |
          | **Branch** | `${{ github.ref_name }}` |
          | **Triggered by** | @${{ github.actor }} |
          | **Timestamp** | $(date -u +%Y-%m-%dT%H:%M:%SZ) |

          ## Configuration

          | Parameter | Value |
          |-----------|-------|
          | **Workload Suite** | `${{ inputs.workload-suite || 'all' }}` |
          | **Duration** | ${{ inputs.duration || '30' }}s per workload |
          | **Concurrency** | ${{ inputs.concurrency || '100' }} connections |
          | **Warmup** | ${{ inputs.warmup || '100' }} requests |

          ## Results

          EOF

          # Count results for each language
          for lang in python node ruby rust; do
            if [ -d "results/benchmark-${lang}-${{ github.run_id }}" ]; then
              count=$(find "results/benchmark-${lang}-${{ github.run_id }}" -name "*.json" 2>/dev/null | wc -l || echo "0")
              echo "- **${lang^}**: ${count} result files" >> benchmark-summary.md
            fi
          done

          cat >> benchmark-summary.md << 'EOF'

          ## Access Results

          Download the artifacts above to access detailed benchmark data including:
          - Framework comparisons
          - Statistical significance analysis
          - Performance metrics (RPS, latency percentiles)
          - Success rates

          ---
          *Generated by Spikard Benchmark Harness*
          EOF

          cat benchmark-summary.md

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary-${{ github.run_id }}
          path: benchmark-summary.md
          retention-days: 90

      - name: Add to job summary
        run: cat benchmark-summary.md >> $GITHUB_STEP_SUMMARY
