name: Benchmarks

'on':
  # Manual trigger only
  workflow_dispatch:
    inputs:
      bindings:
        description: 'Bindings to benchmark (comma-separated: python,node,ruby,rust or "all")'
        required: false
        default: 'all'
        type: string
      workload-suite:
        description: 'Workload suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - json-bodies
          - path-params
          - query-params
          - forms
      duration:
        description: 'Duration per workload in seconds'
        required: false
        default: '30'
        type: string
      concurrency:
        description: 'Concurrent connections per workload'
        required: false
        default: '100'
        type: string
      warmup:
        description: 'Warmup requests per framework'
        required: false
        default: '100'
        type: string
      significance-threshold:
        description: 'Statistical significance threshold (p-value)'
        required: false
        default: '0.05'
        type: string
      run-wasm:
        description: 'Run WASM benchmarks'
        required: false
        default: false
        type: boolean
      run-raw:
        description: 'Run raw (no-validation) baseline comparisons'
        required: false
        default: false
        type: boolean
      run-profile:
        description: 'Enable profiling mode (placeholder for future)'
        required: false
        default: false
        type: boolean
      comparison-frameworks:
        description: 'Override comparison frameworks (comma-separated, leave empty for defaults)'
        required: false
        default: ''
        type: string

concurrency:
  group: benchmarks-${{ github.ref }}
  cancel-in-progress: true

env:
  RUST_BACKTRACE: short
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  CARGO_PROFILE_RELEASE_DEBUG: 0

jobs:
  # Process workflow inputs
  determine-scope:
    name: Process Benchmark Configuration
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      bindings: ${{ steps.scope.outputs.bindings }}
      suite: ${{ steps.scope.outputs.suite }}
      duration: ${{ steps.scope.outputs.duration }}
      concurrency: ${{ steps.scope.outputs.concurrency }}
      warmup: ${{ steps.scope.outputs.warmup }}
      run-wasm: ${{ steps.scope.outputs.run-wasm }}
      run-raw: ${{ steps.scope.outputs.run-raw }}
      run-profile: ${{ steps.scope.outputs.run-profile }}
      retention-days: ${{ steps.scope.outputs.retention-days }}
    steps:
      - name: Process workflow inputs
        id: scope
        run: |
          # Use provided inputs or defaults
          BINDINGS="${{ inputs.bindings || 'all' }}"
          SUITE="${{ inputs.workload-suite || 'all' }}"
          DURATION="${{ inputs.duration || '30' }}"
          CONCURRENCY="${{ inputs.concurrency || '100' }}"
          WARMUP="${{ inputs.warmup || '100' }}"
          RUN_WASM="${{ inputs.run-wasm || 'false' }}"
          RUN_RAW="${{ inputs.run-raw || 'false' }}"
          RUN_PROFILE="${{ inputs.run-profile || 'false' }}"
          RETENTION_DAYS="90"

          echo "bindings=$BINDINGS" >> $GITHUB_OUTPUT
          echo "suite=$SUITE" >> $GITHUB_OUTPUT
          echo "duration=$DURATION" >> $GITHUB_OUTPUT
          echo "concurrency=$CONCURRENCY" >> $GITHUB_OUTPUT
          echo "warmup=$WARMUP" >> $GITHUB_OUTPUT
          echo "run-wasm=$RUN_WASM" >> $GITHUB_OUTPUT
          echo "run-raw=$RUN_RAW" >> $GITHUB_OUTPUT
          echo "run-profile=$RUN_PROFILE" >> $GITHUB_OUTPUT
          echo "retention-days=$RETENTION_DAYS" >> $GITHUB_OUTPUT

          echo "ðŸ“Š Benchmark Configuration:"
          echo "  Bindings: $BINDINGS"
          echo "  Suite: $SUITE"
          echo "  Duration: ${DURATION}s"
          echo "  Concurrency: $CONCURRENCY"
          echo "  Warmup: $WARMUP"
          echo "  Run WASM: $RUN_WASM"
          echo "  Run Raw: $RUN_RAW"
          echo "  Run Profile: $RUN_PROFILE"
          echo "  Retention: ${RETENTION_DAYS} days"

  setup:
    name: Build Benchmark Harness
    needs: determine-scope
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      timestamp: ${{ steps.metadata.outputs.timestamp }}
      short-sha: ${{ steps.metadata.outputs.short-sha }}
      run-python: ${{ steps.check-bindings.outputs.run-python }}
      run-node: ${{ steps.check-bindings.outputs.run-node }}
      run-ruby: ${{ steps.check-bindings.outputs.run-ruby }}
      run-rust: ${{ steps.check-bindings.outputs.run-rust }}
    steps:
      - uses: actions/checkout@v6

      - name: Check which bindings to run
        id: check-bindings
        env:
          BINDINGS: ${{ needs.determine-scope.outputs.bindings }}
        run: |
          echo "run-python=${{ contains(needs.determine-scope.outputs.bindings, 'python') || needs.determine-scope.outputs.bindings == 'all' }}" >> $GITHUB_OUTPUT
          echo "run-node=${{ contains(needs.determine-scope.outputs.bindings, 'node') || needs.determine-scope.outputs.bindings == 'all' }}" >> $GITHUB_OUTPUT
          echo "run-ruby=${{ contains(needs.determine-scope.outputs.bindings, 'ruby') || needs.determine-scope.outputs.bindings == 'all' }}" >> $GITHUB_OUTPUT
          echo "run-rust=${{ contains(needs.determine-scope.outputs.bindings, 'rust') || needs.determine-scope.outputs.bindings == 'all' }}" >> $GITHUB_OUTPUT

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: benchmark-harness

      - name: Build benchmark harness
        run: cargo build --manifest-path tools/benchmark-harness/Cargo.toml --release

      - name: Generate metadata
        id: metadata
        run: |
          echo "timestamp=$(date +%Y-%m-%d-%H-%M)" >> $GITHUB_OUTPUT
          echo "short-sha=${GITHUB_SHA:0:7}" >> $GITHUB_OUTPUT

      - name: Upload harness binary
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-harness-binary
          path: target/release/benchmark-harness
          retention-days: 1

  benchmark-python:
    name: Benchmark Python
    needs: [determine-scope, setup]
    if: needs.setup.outputs.run-python == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - uses: ./.github/actions/build-python-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: python
          spikard-framework: spikard-python
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'fastapi,fastapi-granian,litestar,litestar-granian,robyn' }}
          workload-suite: ${{ needs.determine-scope.outputs.suite }}
          duration: ${{ needs.determine-scope.outputs.duration }}
          concurrency: ${{ needs.determine-scope.outputs.concurrency }}
          warmup: ${{ needs.determine-scope.outputs.warmup }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8100'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-python-${{ needs.setup.outputs.timestamp }}
          path: results/python/**/*.json
          retention-days: 1

  benchmark-node:
    name: Benchmark Node.js
    needs: [determine-scope, setup]
    if: needs.setup.outputs.run-node == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - uses: ./.github/actions/build-node-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: node
          spikard-framework: spikard-node
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'fastify,express,hono' }}
          workload-suite: ${{ needs.determine-scope.outputs.suite }}
          duration: ${{ needs.determine-scope.outputs.duration }}
          concurrency: ${{ needs.determine-scope.outputs.concurrency }}
          warmup: ${{ needs.determine-scope.outputs.warmup }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8200'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-node-${{ needs.setup.outputs.timestamp }}
          path: results/node/**/*.json
          retention-days: 1

  benchmark-ruby:
    name: Benchmark Ruby
    needs: [determine-scope, setup]
    if: needs.setup.outputs.run-ruby == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 50
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - uses: ./.github/actions/build-ruby-binding

      - uses: ./.github/actions/run-benchmarks
        with:
          language: ruby
          spikard-framework: spikard-ruby
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'hanami-api,roda' }}
          workload-suite: ${{ needs.determine-scope.outputs.suite }}
          duration: ${{ needs.determine-scope.outputs.duration }}
          concurrency: ${{ needs.determine-scope.outputs.concurrency }}
          warmup: ${{ needs.determine-scope.outputs.warmup }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8300'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-ruby-${{ needs.setup.outputs.timestamp }}
          path: results/ruby/**/*.json
          retention-days: 1

  benchmark-rust:
    name: Benchmark Rust
    needs: [determine-scope, setup]
    if: needs.setup.outputs.run-rust == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 40
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: benchmark-rust

      - uses: ./.github/actions/run-benchmarks
        with:
          language: rust
          spikard-framework: spikard-rust
          comparison-frameworks: ${{ inputs.comparison-frameworks || 'axum-baseline' }}
          workload-suite: ${{ needs.determine-scope.outputs.suite }}
          duration: ${{ needs.determine-scope.outputs.duration }}
          concurrency: ${{ needs.determine-scope.outputs.concurrency }}
          warmup: ${{ needs.determine-scope.outputs.warmup }}
          significance: ${{ inputs.significance-threshold || '0.05' }}
          port: '8400'

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-rust-${{ needs.setup.outputs.timestamp }}
          path: results/rust/**/*.json
          retention-days: 1

  # WASM Benchmarking (conditional)
  benchmark-wasm:
    name: Benchmark WASM
    needs: [determine-scope, setup]
    if: needs.determine-scope.outputs.run-wasm == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - name: Setup Node.js for WASM runtime
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Rust with WASM target
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: benchmark-wasm

      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

      - name: Build WASM binding
        working-directory: crates/spikard-wasm
        run: |
          wasm-pack build --target nodejs --out-dir dist-node --release
          wasm-pack build --target web --out-dir dist-web --release

      - name: Run WASM benchmarks
        run: |
          echo "WASM benchmarking placeholder - framework integration needed"
          echo "This will benchmark the WASM binding in Node.js and browser environments"
          # TODO: Implement WASM benchmark runner
          # For now, create placeholder results
          mkdir -p results/wasm
          echo '{"framework":"spikard-wasm","status":"placeholder","note":"WASM benchmarking not yet implemented"}' > results/wasm/placeholder.json

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-wasm-${{ needs.setup.outputs.timestamp }}
          path: results/wasm/**/*.json
          retention-days: 1

  # Raw/Baseline Comparisons (validation overhead measurement)
  benchmark-raw:
    name: Benchmark Raw Implementations
    needs: [determine-scope, setup]
    if: needs.determine-scope.outputs.run-raw == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        language: [python, node, ruby]
    steps:
      - uses: actions/checkout@v6

      - uses: ./.github/actions/setup-benchmark-harness
        with:
          timestamp: ${{ needs.setup.outputs.timestamp }}

      - name: Setup language runtime
        uses: ./.github/actions/setup-${{ matrix.language }}
        if: matrix.language != 'ruby'
        continue-on-error: true

      - name: Setup Ruby
        if: matrix.language == 'ruby'
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.2'
          bundler-cache: false

      - name: Run raw benchmarks
        env:
          SUITE: ${{ needs.determine-scope.outputs.suite }}
          DURATION: ${{ needs.determine-scope.outputs.duration }}
          CONCURRENCY: ${{ needs.determine-scope.outputs.concurrency }}
          WARMUP: ${{ needs.determine-scope.outputs.warmup }}
        run: |
          mkdir -p results/raw-${{ matrix.language }}

          # Determine which raw frameworks to test based on language
          case "${{ matrix.language }}" in
            python)
              RAW_FRAMEWORKS="fastapi-raw,litestar-raw,spikard-raw"
              PORT_BASE=8500
              ;;
            node)
              RAW_FRAMEWORKS="fastify-raw,express-raw,hono-raw"
              PORT_BASE=8600
              ;;
            ruby)
              RAW_FRAMEWORKS="roda-raw,hanami-api-raw"
              PORT_BASE=8700
              ;;
          esac

          echo "Testing raw implementations: $RAW_FRAMEWORKS"
          echo "This measures validation overhead by comparing validated vs non-validated implementations"

          # TODO: Implement raw benchmark execution
          # For now, create placeholder indicating raw benchmarks are configured
          echo "{\"language\":\"${{ matrix.language }}\",\"frameworks\":\"$RAW_FRAMEWORKS\",\"status\":\"configured\",\"note\":\"Raw benchmark execution to be implemented\"}" > results/raw-${{ matrix.language }}/summary.json

      - name: Upload raw results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-raw-comparison-${{ matrix.language }}-${{ needs.setup.outputs.timestamp }}
          path: results/raw-${{ matrix.language }}/**/*.json
          retention-days: 1

  aggregate:
    name: Aggregate Results
    needs: [determine-scope, setup, benchmark-python, benchmark-node, benchmark-ruby, benchmark-rust, benchmark-wasm, benchmark-raw]
    if: always() && needs.setup.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: raw-results
          pattern: benchmark-raw-*
          merge-multiple: false

      - name: Download baseline artifacts (if available)
        continue-on-error: true
        run: |
          # Attempt to download baseline results from the main branch
          # This allows comparison with previous runs
          mkdir -p baseline-results
          echo "Baseline comparison will be implemented in future enhancement"

      - name: Create consolidated structure
        env:
          TIMESTAMP: ${{ needs.setup.outputs.timestamp }}
          SHORT_SHA: ${{ needs.setup.outputs.short-sha }}
          SUITE: ${{ needs.determine-scope.outputs.suite }}
          DURATION: ${{ needs.determine-scope.outputs.duration }}
        run: |
          result_dir="benchmark-results-${TIMESTAMP}-${SHORT_SHA}"
          mkdir -p "$result_dir"/{python,node,ruby,rust,wasm,raw}

          # Copy results from each language
          for lang in python node ruby rust wasm; do
            if [ -d "raw-results/benchmark-raw-${lang}-${TIMESTAMP}" ]; then
              find "raw-results/benchmark-raw-${lang}-${TIMESTAMP}" -name "*.json" -exec cp {} "$result_dir/${lang}/" \; 2>/dev/null || true
            fi
          done

          # Copy raw comparison results
          for lang in python node ruby; do
            if [ -d "raw-results/benchmark-raw-comparison-${lang}-${TIMESTAMP}" ]; then
              mkdir -p "$result_dir/raw/${lang}"
              find "raw-results/benchmark-raw-comparison-${lang}-${TIMESTAMP}" -name "*.json" -exec cp {} "$result_dir/raw/${lang}/" \; 2>/dev/null || true
            fi
          done

          # Create enhanced metadata JSON
          cat > "$result_dir/metadata.json" << EOF
          {
            "workflow_run": {
              "run_id": "${{ github.run_id }}",
              "run_number": "${{ github.run_number }}",
              "triggered_by": "${{ github.actor }}",
              "event_type": "manual",
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            },
            "repository": {
              "name": "${{ github.repository }}",
              "ref": "${{ github.ref }}",
              "sha": "${{ github.sha }}",
              "short_sha": "${SHORT_SHA}"
            },
            "configuration": {
              "bindings": "${{ needs.determine-scope.outputs.bindings }}",
              "workload_suite": "${SUITE}",
              "duration": "${DURATION}",
              "concurrency": "${{ needs.determine-scope.outputs.concurrency }}",
              "warmup": "${{ needs.determine-scope.outputs.warmup }}",
              "significance_threshold": "${{ inputs.significance-threshold || '0.05' }}",
              "run_wasm": "${{ needs.determine-scope.outputs.run-wasm }}",
              "run_raw": "${{ needs.determine-scope.outputs.run-raw }}",
              "run_profile": "${{ needs.determine-scope.outputs.run-profile }}"
            }
          }
          EOF

          echo "Created consolidated results in $result_dir"
          find "$result_dir" -type f

      - name: Upload consolidated results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ needs.setup.outputs.timestamp }}-${{ needs.setup.outputs.short-sha }}
          path: benchmark-results-*/**
          retention-days: ${{ needs.determine-scope.outputs.retention-days }}

  report:
    name: Generate Report
    needs: [determine-scope, setup, aggregate]
    if: always() && needs.aggregate.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6

      - name: Download consolidated results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ needs.setup.outputs.timestamp }}-${{ needs.setup.outputs.short-sha }}
          path: results

      - name: Analyze results and generate summary
        id: analyze
        env:
          SUITE: ${{ needs.determine-scope.outputs.suite }}
          DURATION: ${{ needs.determine-scope.outputs.duration }}
        run: |
          # Count result files and calculate basic metrics
          PYTHON_COUNT=$(find results/benchmark-results-*/python -name "*.json" 2>/dev/null | wc -l || echo "0")
          NODE_COUNT=$(find results/benchmark-results-*/node -name "*.json" 2>/dev/null | wc -l || echo "0")
          RUBY_COUNT=$(find results/benchmark-results-*/ruby -name "*.json" 2>/dev/null | wc -l || echo "0")
          RUST_COUNT=$(find results/benchmark-results-*/rust -name "*.json" 2>/dev/null | wc -l || echo "0")
          WASM_COUNT=$(find results/benchmark-results-*/wasm -name "*.json" 2>/dev/null | wc -l || echo "0")
          RAW_COUNT=$(find results/benchmark-results-*/raw -name "*.json" 2>/dev/null | wc -l || echo "0")

          echo "python-count=$PYTHON_COUNT" >> $GITHUB_OUTPUT
          echo "node-count=$NODE_COUNT" >> $GITHUB_OUTPUT
          echo "ruby-count=$RUBY_COUNT" >> $GITHUB_OUTPUT
          echo "rust-count=$RUST_COUNT" >> $GITHUB_OUTPUT
          echo "wasm-count=$WASM_COUNT" >> $GITHUB_OUTPUT
          echo "raw-count=$RAW_COUNT" >> $GITHUB_OUTPUT

      - name: Create enhanced summary report
        env:
          SUITE: ${{ needs.determine-scope.outputs.suite }}
          DURATION: ${{ needs.determine-scope.outputs.duration }}
          CONCURRENCY: ${{ needs.determine-scope.outputs.concurrency }}
          WARMUP: ${{ needs.determine-scope.outputs.warmup }}
          PYTHON_COUNT: ${{ steps.analyze.outputs.python-count }}
          NODE_COUNT: ${{ steps.analyze.outputs.node-count }}
          RUBY_COUNT: ${{ steps.analyze.outputs.ruby-count }}
          RUST_COUNT: ${{ steps.analyze.outputs.rust-count }}
          WASM_COUNT: ${{ steps.analyze.outputs.wasm-count }}
          RAW_COUNT: ${{ steps.analyze.outputs.raw-count }}
        run: |
          cat > benchmark-summary.md << 'EOF'
          # ðŸ“Š Spikard Benchmark Results

          ## Run Information

          | Attribute | Value |
          |-----------|-------|
          | **Run ID** | [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) |
          | **Trigger** | Manual |
          | **Commit** | [`${{ needs.setup.outputs.short-sha }}`](https://github.com/${{ github.repository }}/commit/${{ github.sha }}) |
          | **Branch** | `${{ github.ref_name }}` |
          | **Triggered by** | @${{ github.actor }} |
          | **Timestamp** | $(date -u +%Y-%m-%dT%H:%M:%SZ) |

          ## Configuration

          | Parameter | Value |
          |-----------|-------|
          | **Workload Suite** | `${SUITE}` |
          | **Duration** | ${DURATION}s per workload |
          | **Concurrency** | ${CONCURRENCY} connections |
          | **Warmup** | ${WARMUP} requests |
          | **WASM Enabled** | ${{ needs.determine-scope.outputs.run-wasm }} |
          | **Raw Comparison** | ${{ needs.determine-scope.outputs.run-raw }} |

          ## Results Summary

          | Language | Result Files |
          |----------|--------------|
          | Python   | ${PYTHON_COUNT} |
          | Node.js  | ${NODE_COUNT} |
          | Ruby     | ${RUBY_COUNT} |
          | Rust     | ${RUST_COUNT} |
          | WASM     | ${WASM_COUNT} |
          | Raw      | ${RAW_COUNT} |

          ## Detailed Results

          Full benchmark results are available in the workflow artifacts:
          - **Artifact Name:** `benchmark-results-${{ needs.setup.outputs.timestamp }}-${{ needs.setup.outputs.short-sha }}`
          - **Retention:** ${{ needs.determine-scope.outputs.retention-days }} days

          ### What's Included

          - âœ… Framework comparisons for each language binding
          - âœ… Statistical significance analysis
          - âœ… Performance metrics (RPS, latency percentiles)
          EOF

          if [ "${{ needs.determine-scope.outputs.run-raw }}" = "true" ]; then
            cat >> benchmark-summary.md << 'EOF'
          - âœ… Validation overhead analysis (validated vs raw implementations)
          EOF
          fi

          if [ "${{ needs.determine-scope.outputs.run-wasm }}" = "true" ]; then
            cat >> benchmark-summary.md << 'EOF'
          - âœ… WASM binding performance metrics
          EOF
          fi

          cat >> benchmark-summary.md << 'EOF'

          ### Analysis Notes

          **Interpreting Results:**
          - Higher RPS (requests per second) is better
          - Lower latency percentiles (p50, p95, p99) are better
          - Success rate should be 1.0 (100%)
          - Statistical significance (p < 0.05) indicates reliable performance differences

          **Validation Overhead:**
          Raw implementations skip input validation to measure the overhead of Spikard's type-safe validation layer.
          Compare validated vs raw results to quantify the cost of safety.

          ---
          *Generated by Spikard Benchmark Harness*
          EOF

          cat benchmark-summary.md

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary-${{ needs.setup.outputs.timestamp }}
          path: benchmark-summary.md
          retention-days: ${{ needs.determine-scope.outputs.retention-days }}

      - name: Add to job summary
        run: cat benchmark-summary.md >> $GITHUB_STEP_SUMMARY
