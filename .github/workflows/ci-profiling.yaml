name: CI - Profiling

on:
  workflow_dispatch:
    inputs:
      suite:
        description: Benchmark suite to run
        required: false
        default: all
      duration:
        description: Duration per workload (seconds)
        required: false
        default: "30"
      concurrency:
        description: Concurrent clients
        required: false
        default: "100"
      warmup:
        description: Warmup time (seconds)
        required: false
        default: "10"
  push:
    branches: [benchmarks, profiling]
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/ci-profiling.yaml'

concurrency:
  group: ci-spikard-${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  RUST_BACKTRACE: short
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  CARGO_PROFILE_DEV_DEBUG: 0
  BENCH_SUITE: ${{ github.event_name == 'workflow_dispatch' && inputs.suite || 'all' }}
  BENCH_DURATION: ${{ github.event_name == 'workflow_dispatch' && inputs.duration || '30' }}
  BENCH_CONCURRENCY: ${{ github.event_name == 'workflow_dispatch' && inputs.concurrency || '100' }}
  BENCH_WARMUP: ${{ github.event_name == 'workflow_dispatch' && inputs.warmup || '10' }}

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  build-harness:
    name: Build Benchmark Harness
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: build-harness
          use-sccache: 'false'

      - name: Build benchmark-harness
        run: cargo build --manifest-path tools/benchmark-harness/Cargo.toml --release

      - name: Install oha load generator
        run: cargo install oha --locked

      - name: Install py-spy profiler
        run: cargo install py-spy --locked

      - name: Package benchmark tools
        run: |
          mkdir -p artifacts/bin
          cp target/release/benchmark-harness artifacts/bin/benchmark-harness
          cp ~/.cargo/bin/oha artifacts/bin/oha
          cp ~/.cargo/bin/py-spy artifacts/bin/py-spy
          chmod +x artifacts/bin/benchmark-harness artifacts/bin/oha artifacts/bin/py-spy

      - name: Upload benchmark tools artifact
        uses: actions/upload-artifact@v6
        with:
          name: bench-tools
          path: artifacts/bin/
          retention-days: 1
          if-no-files-found: error

  bench-spikard-rust-validation:
    name: Benchmark spikard-rust-validation
    runs-on: ubuntu-latest
    needs: build-harness
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark tools
        uses: ./.github/actions/setup-benchmark-tools
        with:
          artifact-name: bench-tools
          extra-binaries: "py-spy"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: bench-rust
          use-sccache: 'false'

      - name: Build spikard-rust-validation benchmark app
        working-directory: tools/benchmark-harness/apps/spikard-rust-validation
        run: cargo build --release

      - name: Create results directory
        run: mkdir -p results/spikard-rust-validation

      - name: Run benchmarks
        run: |
          benchmark-harness profile \
            --framework spikard-rust-validation \
            --app-dir tools/benchmark-harness/apps/spikard-rust-validation \
            --suite "$BENCH_SUITE" \
            --duration "$BENCH_DURATION" \
            --concurrency "$BENCH_CONCURRENCY" \
            --warmup "$BENCH_WARMUP" \
            --profiler rust \
            --output results/spikard-rust-validation/profile.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-spikard-rust-validation
          path: results/spikard-rust-validation/
          retention-days: 30

  bench-spikard-python-validation:
    name: Benchmark spikard-python-validation
    runs-on: ubuntu-latest
    needs: build-harness
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark tools
        uses: ./.github/actions/setup-benchmark-tools
        with:
          artifact-name: bench-tools
          extra-binaries: "py-spy"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: bench-python-binding
          use-sccache: 'false'

      - name: Setup Python
        uses: ./.github/actions/setup-python-env
        with:
          python-version: "3.14"
          cache-prefix: bench-python

      - name: Build spikard-python-validation binding
        uses: ./.github/actions/build-python-binding

      - name: Install benchmark app dependencies
        working-directory: tools/benchmark-harness/apps/spikard-python-validation
        run: uv run pip install -e .

      - name: Create results directory
        run: mkdir -p results/spikard-python-validation

      - name: Allow py-spy ptrace
        run: |
          sudo sysctl -w kernel.yama.ptrace_scope=0 || { echo "WARNING: unable to relax ptrace_scope for py-spy" >&2; true; }

      - name: Run benchmarks
        run: |
          benchmark-harness profile \
            --framework spikard-python-validation \
            --app-dir tools/benchmark-harness/apps/spikard-python-validation \
            --suite "$BENCH_SUITE" \
            --duration "$BENCH_DURATION" \
            --concurrency "$BENCH_CONCURRENCY" \
            --warmup "$BENCH_WARMUP" \
            --profiler python \
            --output results/spikard-python-validation/profile.json
        env:
          SPIKARD_METRICS_FILE: ${{ github.workspace }}/results/spikard-python-validation/python-metrics.json
          SPIKARD_METRICS_DEBUG: "1"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-spikard-python-validation
          path: results/spikard-python-validation/
          retention-days: 30

  bench-spikard-node-validation:
    name: Benchmark spikard-node-validation
    runs-on: ubuntu-latest
    needs: build-harness
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark tools
        uses: ./.github/actions/setup-benchmark-tools
        with:
          artifact-name: bench-tools
          extra-binaries: "py-spy"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: bench-node-binding
          use-sccache: 'false'

      - name: Setup Node Workspace
        uses: ./.github/actions/setup-node-workspace
        with:
          node-version: "24"

      - name: Build spikard-node-validation binding
        working-directory: packages/node
        run: pnpm build

      - name: Rebuild workspace after native build
        run: pnpm install --frozen-lockfile

      - name: Create results directory
        run: mkdir -p results/spikard-node-validation

      - name: Run benchmarks
        run: |
          benchmark-harness profile \
            --framework spikard-node-validation \
            --app-dir tools/benchmark-harness/apps/spikard-node-validation \
            --suite "$BENCH_SUITE" \
            --duration "$BENCH_DURATION" \
            --concurrency "$BENCH_CONCURRENCY" \
            --warmup "$BENCH_WARMUP" \
            --profiler node \
            --output results/spikard-node-validation/profile.json
        env:
          SPIKARD_NODE_METRICS_FILE: ${{ github.workspace }}/results/spikard-node-validation/node-metrics.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-spikard-node-validation
          path: results/spikard-node-validation/
          retention-days: 30

  bench-spikard-ruby-validation:
    name: Benchmark spikard-ruby-validation
    runs-on: ubuntu-latest
    needs: build-harness
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark tools
        uses: ./.github/actions/setup-benchmark-tools
        with:
          artifact-name: bench-tools
          extra-binaries: "py-spy"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: bench-ruby-binding
          use-sccache: 'false'

      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: "3.4"
          bundler: "4.0.0"
          bundler-cache: false

      - name: Build spikard-ruby-validation binding
        uses: ./.github/actions/build-ruby-binding

      - name: Install benchmark app dependencies
        working-directory: tools/benchmark-harness/apps/spikard-ruby-validation
        run: bundle install

      - name: Create results directory
        run: mkdir -p results/spikard-ruby-validation

      - name: Run benchmarks
        run: |
          benchmark-harness profile \
            --framework spikard-ruby-validation \
            --app-dir tools/benchmark-harness/apps/spikard-ruby-validation \
            --suite "$BENCH_SUITE" \
            --duration "$BENCH_DURATION" \
            --concurrency "$BENCH_CONCURRENCY" \
            --warmup "$BENCH_WARMUP" \
            --profiler ruby \
            --output results/spikard-ruby-validation/profile.json
        env:
          SPIKARD_RUBY_METRICS_FILE: ${{ github.workspace }}/results/spikard-ruby-validation/ruby-metrics.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-spikard-ruby-validation
          path: results/spikard-ruby-validation/
          retention-days: 30

  bench-spikard-php-validation:
    name: Benchmark spikard-php-validation
    runs-on: ubuntu-latest
    needs: build-harness
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark tools
        uses: ./.github/actions/setup-benchmark-tools
        with:
          artifact-name: bench-tools
          extra-binaries: "py-spy"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-key-prefix: bench-php-binding
          use-sccache: 'false'

      - name: Setup LLVM/Clang
        uses: ./.github/actions/setup-llvm-clang

      - name: Setup OpenSSL
        uses: ./.github/actions/setup-openssl

      - name: Setup PHP
        uses: shivammathur/setup-php@v2
        with:
          php-version: "8.4"
          tools: composer
          coverage: none
          extensions: fileinfo, excimer

      - name: Verify PHP profiler extension
        run: php -m | grep -i excimer

      - name: Install PHP development headers
        run: bash scripts/ci/php/install-dev-headers-linux.sh

      - name: Build spikard-php extension
        run: cargo build --release -p spikard-php --features extension-module

      - name: Enable PHP extension
        run: |
          EXT_FILE=$(find target/release -name "libspikard_php.*" -o -name "spikard_php.*" | head -1)
          if [ -z "$EXT_FILE" ]; then
            echo "Failed to locate built PHP extension artifact under target/release" >&2
            exit 1
          fi
          EXT_DIR=$(php-config --extension-dir)
          sudo cp "$EXT_FILE" "$EXT_DIR/spikard_php.so"
          echo "extension=spikard_php.so" | sudo tee /etc/php/8.4/cli/conf.d/99-spikard.ini
          php -m | grep spikard

      - name: Install PHP package dependencies
        working-directory: packages/php
        run: composer install

      - name: Install benchmark app dependencies
        working-directory: tools/benchmark-harness/apps/spikard-php-validation
        run: composer install

      - name: Create results directory
        run: mkdir -p results/spikard-php-validation

      - name: Run benchmarks
        run: |
          benchmark-harness profile \
            --framework spikard-php-validation \
            --app-dir tools/benchmark-harness/apps/spikard-php-validation \
            --suite "$BENCH_SUITE" \
            --duration "$BENCH_DURATION" \
            --concurrency "$BENCH_CONCURRENCY" \
            --warmup "$BENCH_WARMUP" \
            --profiler php \
            --output results/spikard-php-validation/profile.json
        env:
          SPIKARD_PHP_PROFILE_OUTPUT: ${{ github.workspace }}/results/spikard-php-validation/php.speedscope.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-spikard-php-validation
          path: results/spikard-php-validation/
          retention-days: 30
